{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gni 논문지 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 업데이트 전 raw_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import *\n",
    "\n",
    "corpus_root = \"C:/Users/KimMinyoung/nltk_data/corpora/Genomics-Informatics-Corpus-master/Genomics-Informatics-Corpus-master/raw_text2/\"\n",
    "GNICorpus = PlaintextCorpusReader(corpus_root, '.*\\.txt',encoding ='utf-16')\n",
    "giRaw = GNICorpus.raw('gni-7-3-148.txt')\n",
    "raw = GNICorpus.raw(GNICorpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Genomics & Informatics   Vol. 7(3) 148-151, September 2009\\r\\n\\r\\nThe Effect of Increasing Control-to-case Ratio on Statistical Power in a Simulated Case-control SNP Association Study\\r\\n\\r\\nMoonsu Kang, Sunhee Choi and InSong Koh*\\r\\n\\r\\nDepartment of Physiology, College of Medicine, Han- yang University, Seoul 133-791, Korea\\r\\n\\r\\nAbstract\\r\\n\\r\\nGenerally, larger sample size leads to a greater stat- istical power to detect a significant difference. We may increase the sample size for both case and control in order to obtain greater power. However, it is often the case that increasing sample size for case is not feasible for a variety of reasons. In order to look at change in power as the ratio of control to case varies (1:1 to 4:1), we conduct association tests with simulated data gen- erated by PLINK. The simulated data consist of 50 dis- ease SNPs and 300 non-disease SNPs and we compute powers for disease SNPs. Genetic Power Calculator was used for computing powers with varying the ratio of control to case (1:1, 2:1, 3:1, 4:1). In this study, we show that gains in statistical power resulting from in- creasing the ratio of control to case are substantial for the simulated data. Similar results might be expected for real data.\\r\\n\\r\\nKeywords: association study, ratio of control to case, si- mulated data, SNP, statistical power\\r\\n\\r\\nIntroduction\\r\\n\\r\\nThe power of a study is the probability that the test will\\r\\n\\r\\nminimum odds ratio declared to be significant and cor- relation coefficient for exposure between cases and their matched controls. Hennessy S described the effect of increasing the ratio of control to case for different val- ues of correlation coefficients and prevalence among controls in matched case-control studies (Hennessy S et al ., 1999). For a detailed review of power and sample size computation in either genetic studies or genetic ep- idemiology, please refer to Shork et al . (2002), Ambro- sius et al . (2004), De La Vega et al . (2005), and Burton et al . (2009). In our study, we may focus on how sample size affects statistical power, given a set of population parameters.\\r\\n\\r\\nGenerally, increase in sample size for both case and control leads to increase in statistical power. There are some situations, however, where increasing sample size for case is not available. For example, in rare diseases, the cost of including additional controls is low whereas that of including cases is high. In such instances, we in- crease sample size for control only and then see if the effect on statistical power is the same as that obtained when the sample size for both case and control increases. Specifically, we examine if increase in the ra- tio of control to case has an effect on increasing power. We simulate SNP data as below and assess the effect of the ratio of control to case on statistical power.\\r\\n\\r\\nFig. 1. Type I error and Type II error.\\r\\n\\r\\nDecision (H 0 )\\r\\n\\r\\nReject Not reject\\r\\n\\r\\nreject a null hypothesis that is in fact false. As power increases, the probability of a Type II error (false neg- ative rate = β ) decreases (Fig. 1). Therefore power is 1- β . Decreasing β error is equivalent to increasing stat- istical power (Fig. 2).\\r\\n\\r\\nH 0\\r\\n\\r\\nTrue\\r\\n\\r\\nFalse (=H 1 )\\r\\n\\r\\nα\\r\\n\\r\\n(Type Ⅰ error, 1- α\\r\\n\\r\\nfalse positive)\\r\\n\\r\\nβ\\r\\n\\r\\n1- β\\r\\n\\r\\n(Type Ⅱ error,\\r\\n\\r\\n(Power)\\r\\n\\r\\nfalse negative)\\r\\n\\r\\nPower depends on several factors such as preva- lence, magnitude of effect, sample size, and required level of statistical significance α . When computing stat- istical power in matched case-control studies (Dupont, 1988), we need to know a pre-specified type I error rate, the ratio of control to case, estimated number of cases, the prevalence of exposure in the control group,\\r\\n\\r\\n*Corresponding author: E-mail insong@hanyang.ac.kr Tel +82-2-2220-0615, Fax +82-2-2281-3603\\r\\n\\r\\nAccepted 1 September 2009                                 Fig. 2. Statistical power.\\r\\n\\r\\n-----------------------------------------------------Page 1-----------------------------------------------------\\r\\n\\ufeff\\x0c\\r\\nEffect of the Ratio of Control to Case on Power    149\\r\\n\\r\\nTable 1. Simulated data\\r\\n\\r\\n\\u3000           ID         null_1        null_2         …        null_300      disease_1     disease_2        …       disease_50 Case              1          d D          D D          …          d D         d D         d d          …          d d\\r\\n\\r\\n2          D D          d D          …          d D         d d         d d          …          D d 3          d D          d D          …          D D         d D         d d          …          d d 4          D D         D D          …          d d         d d         d d          …          d d\\r\\n\\r\\n…           …           …           …           …           …           …           …           … 50          D D         D D          …          D D         d D         d d          …          d d Control            51          D D         D D          …          D D         d D         d d          …          d D\\r\\n\\r\\n52          D D         D D          …          D D         d d         D d          …          d d 53          D D         D D          …          d D         d d         d d          …          D d 54          d D          D D          …          D D         d D         d d          …          d d\\r\\n\\r\\n…           …           …           …           …           …           …           …           … 250          D D          d D          …          d d         d D         D d          …          D D\\r\\n\\r\\nNull, non-disease SNP; Disease, disease SNP; d, minor allele; D, major allele.\\r\\n\\r\\nTable 2. The number of significant SNP for each ratio of control-case (p ＜ 0.05) [Allele model]\\r\\n\\r\\ncontrol : case           1:1 2:1 3:1 4:1\\r\\n\\r\\n(50:50) (100:50) (150:50) (200:50)\\r\\n\\r\\nSignificant\\r\\n\\r\\nSNP\\r\\n\\r\\nTotal                50       53       54       52 Disease SNP         31       35       36       39 Non-disease SNP      19       18       18       13\\r\\n\\r\\nMethods\\r\\n\\r\\nPLINK (Purcell et al ., 2007) was used for generating si- mulated data with 50 disease SNPs and 300 non-dis- ease SNPs (Table 1). In this data, we fixed the sample size for case as 50 but the sample size for control size varies from 50, 100, 150 to 200 in order to investigate the effect of the ratio of control to case on power. We set prevalence as 0.5000, 0.3333, 0.2500, and 0.2000 for four models, respectively. First, assuming allele mod- el, we computed the number of significant SNPs for dis- ease SNPs, non-disease SNPs, and overall SNPs as the ratio of control to case changes. We also computed the estimated average power which is equal to E(S)/m (eq1), where S is the number of SNPs declared to be sig- nificant among disease SNPs and m is the number of disease SNPs. Second, we examined the power for each disease SNP using Genetic Power Calculator (Purcell S et al ., 2003) in order to see how change in the ratio of control to case affects the power for the ge- netic models (allele, genotype, dominant, and recessive).\\r\\n\\r\\nResults and Discussion\\r\\n\\r\\nTable 2 shows that the number of significant SNPs in- creases as the ratio of control to case for allele model rises from 1:1 to 3:1. The increase in the ratio of control\\r\\n\\r\\nFig. 3. Average power for disease SNP (p ＜ 0.05) [Allele model].\\r\\n\\r\\nto case for disease SNPs leads to increase in number of significant SNPs, while the increase in the ratio of control to case for non-disease SNPs leads to decrease in the number. Therefore, the number of significant SNPs decreases when the ratio of control to case in- creases from 3:1 to 4:1. We might expect that the gain in average power shown in eq1 increases as the ratio of control to case increases, since the effect of the ratio of control to case for disease SNPS on the number of significant SNPs is substantial. Assuming allele model, the average power shown in eq1 increases for disease SNPs as the ratio of control to case increases (Fig. 3). In other words, the power curve tends to increase gradually.\\r\\n\\r\\nOn the other hand, in regard to computing statistical power for each disease SNP, for example, the disease SNP 3 (Fig. 4) is on the increase for all models. The dis- ease SNP 17, the power increases for all models except dominant model (Fig. 5). The SNP 30 (Fig. 6) is on the increase for all models. The disease SNP 34 has the in- creasing pattern except recessive model (Fig. 7). The SNP 45 (Fig. 8) is on the increase for all models. We\\r\\n\\r\\n-----------------------------------------------------Page 2-----------------------------------------------------\\r\\n\\ufeff\\x0c\\r\\n150 Genomics & Informatics Vol. 7(3) 148-151, September 2009\\r\\n\\r\\nFig. 4. Statistical power for Disease SNP 3.\\r\\n\\r\\nFig. 7. Statistical power for Disease SNP 34.\\r\\n\\r\\nFig. 5. Statistical power for Disease SNP 17.\\r\\n\\r\\nFig. 6. Statistical power for Disease SNP 30.\\r\\n\\r\\nuse Chi-square test for testing allelic association and genotype analyses. We do not show all other disease SNPs in the paper but in general, statistical power for most disease SNPs is likely to increase by increase in the ratio of control to case. In summary, we show that\\r\\n\\r\\nFig. 8. Statistical power for Disease SNP 45.\\r\\n\\r\\ngiven other factors such as prevalence, magnitude of ef- fect, and required level of statistical significance α , sig- nificant increase in statistical power can be obtained by increasing the ratio of control to case. Henceforth, in- vestigators conducting such a study where cases are limited might consider increase in the ratio of control to case. Further investigation may be needed for real data. And other factors which affect the power need to be considered.\\r\\n\\r\\nAcknowledgements\\r\\n\\r\\nThis research was supported by grants from MOHWF, Korea. (00-PJ3-PG6-GN02-0002)\\r\\n\\r\\nReferences\\r\\n\\r\\nAmbrosius, W.T., Lange, E.M., and Langefeld, C.D. (2004). Power for genetic association studies with random allele frequencies and genotype distributions. Am. J. Hum. Genet. 74, 683-693.\\r\\n\\r\\nBurton, P.R., Hansell, A.L., Fortier I., Manolio, T.A., Khoury, M.J., Little, J., and Elliott, P. (2009). Size matters: just\\r\\n\\r\\n-----------------------------------------------------Page 3-----------------------------------------------------\\r\\n\\ufeff\\x0c\\r\\nhow big is BIG? Quantifying realistic sample size require- ments for human genome epidemiology. Int. J. Epidemiol. 38, 263-273.\\r\\n\\r\\nDe La Vega, F.M., Gordon, D., Su, X., Scafe, C., Isaac, H., Gilbert, D.A., and Spier, E.G. (2005). Power and sample size calculations for genetic case/control studies using gene-centric SNP maps: Application to human chromo- somes 6, 21, and 22 in three populations. Hum. Hered. 60, 43-60.\\r\\n\\r\\nDupont, W.D. (1988). Power calculations for matched\\r\\n\\r\\ncase-control studies. Biometrics 44, 1157-1168.\\r\\n\\r\\nDupont, W.D., and Plummer, W.D.Jr. (1990). Power and sample size calculations. A review and computer program. Control Clin. Trials. 11, 116-128.\\r\\n\\r\\nHennessy, S., Bilker, W.B., Berlin, J.A., and Storm B.L. (1999). Factors influencing the optimal control to case ra- tio in matched case-control studies. Am. J. Epidemiol. 149, 195-197.\\r\\n\\r\\nEffect of the Ratio of Control to Case on Power    151\\r\\n\\r\\nLewis, C.M. (2002). Genetic association studies: design, analysis and interpretation. Brief Bioinform. 3, 144-153. Park, K., and Kim, H. (2007). A review of power and sam- ple size estimation in genomewide association studies. J. Prev. Med. Public Health. 40, 114-121.\\r\\n\\r\\nPurcell, S., Cherny, S.S., and Sham, P.C. (2003). Genetic power calculator: design of linkage and association ge- netic mapping studies of complex traits. Bioinformatics 19, 149-150.\\r\\n\\r\\nPurcell, S., Neale, B., Todd-Brown, K., Thomas, L., Ferreira, M.A.R., Bender, D., Maller, J., Sklar, P., De Bakker, P.I.W., Daily, M.J., and Sham, P.C. (2007). PLINK: A toolset for whole-genome association and pop- ulation-based linkage analysis. Am. J. Hum. Genet. 81, 559-575.\\r\\n\\r\\nSchork, N.J. (2002). Power calculation for genetic associa- tion studies using estimated probability distributions. Am. J. Hum. Genet. 70, 1480-1489.\\r\\n\\r\\n-----------------------------------------------------Page 4-----------------------------------------------------\\r\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 : 단어 사이가 -로 끊어짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Han- yang',\n",
       " 'stat- istical',\n",
       " 'gen- erated',\n",
       " 'dis- ease',\n",
       " 'in- creasing',\n",
       " 'si- mulated',\n",
       " 'cor- relation',\n",
       " 'val- ues',\n",
       " 'ep- idemiology',\n",
       " 'Ambro- sius',\n",
       " 'in- crease',\n",
       " 'ra- tio',\n",
       " 'neg- ative',\n",
       " 'stat- istical',\n",
       " 'preva- lence',\n",
       " 'stat- istical',\n",
       " 'si- mulated',\n",
       " 'dis- ease',\n",
       " 'mod- el',\n",
       " 'dis- ease',\n",
       " 'sig- nificant',\n",
       " 'ge- netic',\n",
       " 'in- creases',\n",
       " 'in- creases',\n",
       " 'dis- ease',\n",
       " 'in- creasing',\n",
       " 'ef- fect',\n",
       " 'sig- nificant',\n",
       " 'in- vestigators',\n",
       " 'require- ments',\n",
       " 'chromo- somes',\n",
       " 'ra- tio',\n",
       " 'sam- ple',\n",
       " 'ge- netic',\n",
       " 'pop- ulation',\n",
       " 'associa- tion']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern1 = re.findall(r'[A-za-z]+- [a-z]+',giRaw)\n",
    "pattern1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hanyang',\n",
       " 'statistical',\n",
       " 'generated',\n",
       " 'disease',\n",
       " 'increasing',\n",
       " 'simulated',\n",
       " 'correlation',\n",
       " 'values',\n",
       " 'epidemiology',\n",
       " 'Ambrosius',\n",
       " 'increase',\n",
       " 'ratio',\n",
       " 'negative',\n",
       " 'statistical',\n",
       " 'prevalence',\n",
       " 'statistical',\n",
       " 'simulated',\n",
       " 'disease',\n",
       " 'model',\n",
       " 'disease',\n",
       " 'significant',\n",
       " 'genetic',\n",
       " 'increases',\n",
       " 'increases',\n",
       " 'disease',\n",
       " 'increasing',\n",
       " 'effect',\n",
       " 'significant',\n",
       " 'investigators',\n",
       " 'requirements',\n",
       " 'chromosomes',\n",
       " 'ratio',\n",
       " 'sample',\n",
       " 'genetic',\n",
       " 'population',\n",
       " 'association']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub(r'- ','',p)for p in pattern1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제: 아래첨자 띄어쓰기 문제 \n",
    "\n",
    "(귀무가설 대립가설기호가 잘못읽혔다. H0 -> H 0 으로 읽힘)\n",
    "\n",
    "모든 아래첨자에서 띄어쓰기가 발생한 것을 확인했다. 다만 모두 찾기는 어려우므로, 위 기호와 같은 것들은 먼저 처리할 수 있을 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H 0', 'H 0', 'H 1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern2 = re.findall(r'[H]+ [0-1]+',giRaw)\n",
    "pattern2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H0', 'H0', 'H1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub(r' ','',p)for p in pattern2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "giRaw2 = GNICorpus.raw('gni-9-1-19.txt')\n",
    "giRaw3 = GNICorpus.raw('gni-9-1-12.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 : 카이스퀘어 기호 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['χ test', 'χ test']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern3= re.findall(r'χ test',giRaw3)\n",
    "pattern3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['χ test', 'χ test', 'χ test', 'χ test', 'χ test']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern3all= re.findall(r'χ test',raw)\n",
    "pattern3all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['χ2 test', 'χ2 test']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub(r'χ test','χ2 test',p)for p in pattern3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 업데이트된 raw_text2 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Standard-based Integration of Heterogeneous Large-scale DNA Microarray Data for Improving Reusability.\\n\\n  Gene Expression Omnibus (GEO) has kept the largest amount of gene-expression microarray data that have grown exponentially.  Microarray data in GEO have been generated in many different formats and often lack standardized annotation and documentation.  It is hard to know if preprocessing has been applied to a dataset or not and in what way.  Standard-based integration of heterogeneous data formats and metadata is necessary for comprehensive data query, analysis and mining.  We attempted to integrate the heterogeneous microarray data in GEO based on Minimum Information About a Microarray Experiment (MIAME) standard.  We unified the data fields of GEO Data table and mapped the attributes of GEO metadata into MIAME elements.  We also discriminated non-preprocessed raw datasets from others and processed ones by using a twostep classification method.  Most of the procedures were developed as semi-automated algorithms with some degree of text mining techniques.  We localized 2,967 Platforms, 4,867 Series and 103,590 Samples with covering 279 organisms, integrated them into a standardbased relational schema and developed a comprehensive query interface to extract.  Our tool, GEOQuest is available at http://www. snubi. org/software/GEOQuest/Keywords: gene expression data, data integration, classificationAfter genome sequencing, DNA microarray analysis has become the most widely used source of genomescale data in the life sciences (Allison et al., 2006; Brazma et al., 2001).  DNA microarray is a highthroughput and dataintensive technology that provides the means of measuring the expression of thousands of genes or proteins simultaneously and brings the unprecedented development in the informatics and analysis aspect (Chaussabel and Sher, 2002; Quackenbush, 2002).  Since this technique provides researchers with comprehensive understanding of biological complex features, it has been used in not only biological but also clinical field.    As many microarray experiments generate large data sets that can contain tens to hundreds of samples, however, it has needed to be managed systematically with computational tools due to their own complexity.  Moreover, requirements for ensuring the scientific integrity of data and sharing the data have given rise to development of public microarray repositories like GEO (Barrett et al., 2007), ArrayExpress (Parkinson et al., 2007), Stanford Microarray Database (SMD) (Gollub et al., 2003), keeping pace with the standardization (Edgar and Barrett, 2006; Perou, 2001).  With appearance of them, the data generated in one laboratory can be available to other researchers and various analytical methods uncover different biological insights.   Especially, NCBI GEO has kept the largest amount of gene expression data which have grown exponentially, currently holdingover 200,000 samples.  Recently, researches to find clinical or biological meaning using GEO data have been actively performed to show its reusability.  Butte et al.  (2006) constructed PhenomeGenome network, Disease Nosology and GeneBehavior Disease through analysis of huge amounts of gene expression data in GEO using Unified Medical Language System (UMLS) (Humphreys et al., 1998).  Yoon et al.  (2006) constructed an application tool to provide the present largescale approach for the analysis of GEO microarray data.    Though GEO has abundant practical possibilities, it does not support the standard format or model but its selfstructured format.  Moreover, it stores the microarray data without distinguishing processed data and raw data.  These factors hinder providing a comprehensive biological analysis environment and future integration of   the external resources and make extracting the desired information difficult.   Currently, some tools have been developed to utilize large-scale GEO data - SeqExpress (Boyle, 2005), GEOQuery (Sean and Meltzer, 2007), ArrayQuest (Argraves et al., 2005).  However, there is a limitation that they can handle only GEO DataSets which are curated by GEO staffs.   To solve the problem and overcome the limitation, we attempted integration of heterogeneous microarray data in GEO using microarray data standard, Minimum Information About a Microarray Experiment (MIAME).  In GEO, it encourages submitters to supply MIAME standard compliant data.  However, it is not related to the format, but rather to the content provided.    We performed data field unification of GEO Data table and distinguished between raw data and transformed data using classification method.  To integrate the data in an efficient and accurate manner, these processes were developed in both manual and semiautomated way (Vita et al., 2006). Biological research increasingly depends on computational analysis of data (Miotto et al., 2005).  A prerequisite for computational analysis is the availability of experimental data in a formalized, structured and machine-readable format.  GEO offers DataSet (GDS) which represents a curated collection of biologically and statistically comparable data for computational analysis method.  However, there is a limitation that GEO data are reassembled mainly for not entire GEO data but a unit of GEO Series (GSE) only.    To overcome the limitation and provide a comprehensive biological analysis environment, we attempted integration of heterogeneous microarray data in GEO using microarray data standard, Minimum Information About a Microarray Experiment (MIAME).  Workflows are as follows (Fig.  1): GEO data localization, Data field unification, Data transformation classification, Standard based integration of heterogeneous microarray data, and GEO data update. GEO is an international repository for gene expression data.  It is developed and maintained by the National Library of Medicine (NLM).  It serves as a public repository for a wide range of highthroughput experimental data like single and dual channel microarraybased experiments measuring mRNA, miRNA, genomic DNA and protein abundance.    In GEO, the basic entity types are Platform, Sample and Series.  Platform includes a summary description of the array (Descriptive information) and a data table defining the array template (Data table).  Each row in the data table corresponds to a single element, and includes sequence annotation and tracking information as   provided by the submitter.  Sample includes a description of the biological source and the experimental protocols to which it was subjected (Descriptive information), and a data table containing hybridization measurements for each element on the corresponding Platform (Data table).  Series defines a set of related Samples considered to be part of a study, and describes the overall study aim and design.  Each of these entities is assigned an accession number that may be used to cite and retrieve the records.   Each entity type has ‘Descriptive information’ for describing each entity’s correspondent information.  Platform and Sample have ‘Data table’ which consists of ‘Header’ and ‘Matrix’.  Header identifies the attributes of each column of Matrix.  Matrix includes each entity’s correspondent data contents. We accessed and downloaded all GEO data on January 22, 2007.  Python language and MySQL DataBase Management System were used in Linux system for localizing GEO database.  Since data in GEO’s File Transfer Protocol were updated late at that time, we used HyperText Markup Language (HTML) documents from which users can see web page.  Accession Display  accession to the data through GEO accession number  was used to automatically download all GEO data.  Using specific HTML tag structure, we localized all GEO data.   To integrate increasingly large volume of GEO data, formalizing loosely-defined format of GEO is an indispensable process.  First, we determined representative fields and constructed mapping tables for Data table of three technology types - ‘spotted DNA/cDNA’, ‘spotted oligonucleotide’, and ‘in situ oligonucleotide’.  The criteria of determining representative fields for data field unification are shown in Table 1.   To construct each mapping table, we adapted a simple text-mining method and performed through manual curation with checking a description and values of each field.  Through the mapping tables, we mapped fields of all GEO Data table into each representative field.  Five database tables are used to store the GEO Data table for Platform, dual channel Sample, and three technology types in single channel Sample.  Ambiguous cases on mapping them are treated by EAV (EntityAttribute Value) table (Johnson et al., 1997) to prevent loss of data caused by the mapping process.    After inserting data into each result table is completed, the four tables of Sample are integrated through the next stepThe GEO database architecture is designed for the efficient capture and storage of heterogeneous highthroughput data sets.  The structure is sufficiently flexible to accommodate evolving state of the art technologies (Barrett and Edgar, 2006).  Consequently, the data have many different styles and comprise varying contents.    Due to the flexibility, GEO stores the data which include the words having the same concept but different spelling or some misspelled words without any control process.  This characteristic increases heterogeneity between data in GEO and makes future integration of this resource with other biological and clinical data difficult.  Data standard is an essential requirement for representation of information to ensure proper semantic integration of heterogeneous data, and also for communication standards to ensure interoperability between disparate data sources (Louie et al., 2007; MartinSanchez et al., 2004).  In microarray data, the introduction of the MIAME standard has been a great success (Rayner et al., 2006).  However, the data format of GEO does not follow the standard.  Therefore, it hinders semantic integration and interoperability between heterogeneous microarray data.  For the reason, we customized the GEO data into MIAME standardbased format.   First, we analyzed attributes of Descriptive information in each GEO entity to understand that which GEO attribute corresponds to which part of MIAME.  Second, we mapped GEO attributes to elements in each part of MIAME and stored them into the database.  Third, we mapped values of some GEO attributes into controlled terms from the MGED Ontology (http://mged. sourceforge.  net/ontologies/index. php).  For example, we performed mapping from ‘technology type’ in Platform to TechnologyType class and from ‘type’ in Series to both MethodologicalDesign class and ExperimentalFactor class.  In case of the ‘type’ in Series, there are the multiple values in one text, we split the text into single values.  In the second and third processes, we performed both a simple text mining method and manual curation with checking a description and values of each field.  Finally, microarray data in which data field unification process is done are stored into one database table according to classification result between loglike transformed data and raw data.  The hypothesis underlying microarray analysis is that the measured intensities for each arrayed gene represent its relative expression level.  Before the levels can be compared appropriately, a number of transformations must be carried out on the data to adjust the measured intensities to facilitate comparisons and to select genes that are significantly differentially expressed (Quackenbush, 2002).   In microarray data analysis, difference between raw data and transformed data affects analysis results importantly.  Moreover, MIAME provides the conceptual structure for the representation of microarray data including raw and processed data (Brazma et al., 2001; Rayner et al., 2006).  However, both raw data and transformed data are stored together in GEO with no separation.  For the reasons, we must distinguish between transformed data and raw data.  Even though submitters are encouraged to describe about a process of Sample ‘Value’ field which indicates final expression value measurements, a large part of ‘Value’ fields have not only an ambiguous description or no description but even wrong description (Table 2).    In this paper, we assumed that the log transformation are mainly used among the several transformation processes and attempted to distinguish raw data and loglike transformed data.  We treated some transformation method as loglike transformation (Table 3).  We propose a model for data processing classification using machine learning techniques. Selection of a training data set For most classification study, a training data set consisting of records whose class labels are known must be provided.  It is necessary to select training data properly because the training data set is used to build a classification model.  First, we performed simple random sampling and extracted 200 GEO Samples.  Sampling is a commonly used approach for selecting a subset of the data objects to be analyzed.  Next, we classify data manually in compliance with three criteria for giving the correct class to data.   • Description of ‘Value’ field in GEO Sample  • Range of data distribution  • Symmetricity of distribution  In the classification process, guarantee of data quality is very important factor.  We trimmed twotailed 5% values of the distribution in each Sample to remove outliers before selection of training data.  We extracted 188 GEO Samples for three criteria and emailed to data submitters of 12 Samples, the rest of the data set.  As a result, 190 Samples are determined as training set (96 Loglike transformed data, and 94 Notlog transformed data).  Learning process for classification modelWe determined the features that can explain a difference between two classes to create a classification model.   The first one is the difference of the skewness values between original distribution and its loglike transformed distribution (DSD).  Skewness is a measure of the asym  metry of the data of the probability distribution.     where χi is each value of the distribution and x represents the mean value of the distribution.  n is the number of values in the distribution.   If expression values of raw data are loglike transformed, the skewness of original distribution is changed remarkably (Fig.  2).  However, if those of loglike transformed data are log-like transformed again, the skewness is changed slightly.  This characteristic makes the feature available for the classification model.   Second, a maximum value of data (MD) is concerned.  Common image scanners generate typically 16bit Tagged Information File Format (TIFF) images.  Therefore, the log-like transformed data have rarely over 16.  On the other hand, raw data can have values more than thousands.  Since the maximum value can be a good factor for the classification process, we include this feature.   To determine the classification model, we adopted the logistic regression method with these features.   where y is defined as above and xk,i is the value of features of ith GEO Sample data.  k is the number of features and n is the number of GEO Samples.  The logistic regression has several strengths.  1) It is not restricted to data.  2) It can represent the model easily.  3) It can classify the data into each group with ease.  4) It finds the best explanatory variable.  We can predict the class, log-like transformed or not for any GEO Sample data using the output computed by the model.  If y is the positive value, the data is classified as loglike transformed data.  If y is the negative value, it is classified as not log-like transformed data. Set-wise validation of classification result10 Sample data excluded from 200 Sample data were used to validate the classification model.  We performed classification of 10 data and case-study of its result.  A GEO Series is a group of related GEO Samples.  Thus, we can assume that Samples in a Series should be classified as same class.  However, it is found that Samples are classified with different class in a Series.  To solve this case, we adopted simple voting method and classified Samples in a Series as a class with which Samples are classified more. As an amount of GEO data grows exponentially, we need to update the data continuously.  For incremental update, we extract a list of GEO accession numbers and release date in our database.  Next, if a new data is not in the list, we download, process, and store it into the database.  To trace the update status, accession numbers of updated data and the updated date are written in a log file.   Besides, authority of data in GEO changed often public to private, and vice versa.  We also recorded the accession numbers of the data in the log file.  Update workflow is shown in Fig.  3.  In this section, we focused on constructing the mapping tables for each technology type and each manufacturer.  Through a simple textmining method and manual curation, we mapped the fields which are written in various strings into one representative field.  Table 1 shows the example of words which have various strings for a concept.   There are around 3,000 distinct fields in Platform and Sample Data tables respectively.  Since most of the data fields are recorded irregularly, we have to unify them manually.  On the other hand, the fields of dual channel   in Sample are stored in partially regular pattern.  We performed a simple text mining method (exact match) and obtained a precision of 0.8474 (544/642), a recall of 0.2011 (544/2,705), and the F-measure of 0.3248.  Through its method was not reliable as the result shows, it was helpful to reduce a timeconsuming process.  The unification result is shown in Table 4. We designed a core relational database according to the MIAME standard-based format.  In it, the GEO data is customized according to the result of mapping GEO attributes to elements in each part of MIAME.  A comprehensive view of mapping result is presented in Table 5.   We mapped values of attributes in GEO (‘technology type’ in Platform, ‘type’ in Series, ‘Extracted molecule’ and ‘Label’ in Sample) into terms from MGED Ontology (TechnologyType, MethodologicalDesign and ExperimentalFactor, and LabeledExtract class) respectively.  Like data field unification process, we performed a simple text mining method.  Among 4,495 values in GEO, 2,537 values are mapped to MGED Ontology and 1,958 values are stored as they are.  Other database tables have been implemented in order to collect data regarding all submitters, laboratories or organizations which take part in each experiment and to handle the case that a Sample included in multiple Series.  All data regarding microarray experiment results (Data table) are stored in the additional database tables according to its transformation characteristic.   Finally, we hold 2,967 Platforms, 103,590 Samples (57,052 single channels and 46,538 dual channels) and 4,867 Series in our database.  In the database, we investigated the distribution of organism, source, and extracted molecule in microarray experiments (Table 6).  The view of the results reflects that the most interesting experiments in the present researches have been concerned with Homo sapiens.  Moreover, it shows that the breast tissue is mostly used as a material for experiments and that breast cancer is a matter of primary concern in cancer research.  In a part of Extracted Molecule, the distribution means the data stored in GEO mainly result from arraybased experiments in which researches of transcriptional pattern is accomplished. Classification using Logistic Regression modelThe initial classification is done by logistic regression, which is used when users have a binary dependent variable.  The training data set currently consists of 190   examples.   After learning process, we can obtain the classification model and the contingency table as a learning result (Table 7).  As we can see the result, we obtained a sensitivity of 1.0 (96/96) and a specificity of 1.0 (94/94).  Set-wise validationTo test the classification model, we performed classification for the 10 Sample data excluded from 200 Sample data which were sampled randomly to make training data set.  We assumed that basically, Samples in a Series have same classification results.  Among the 10 classification results, however, we found a questionable result.  Though two Samples are included in same Series, one was classified as Log-transformed data and the other was not.  For the reason, we considered Series as a set and attempted the setwise validation.    We applied our classification model to entire data set and extracted the data which correspond to questionable case.  Entire data consist of 3,911 Series which include 97,026 Samples.  The reason why the number of entire Samples is not 103.590 is that GEO Values of 6,564 Samples have only ‘null’ string or zero value.  As a result, we found 103 Series which include 4,876 Samples.  These data were classified by humans to validate the classification model (Table 8).   To solve this problem, we applied a simple voting method.  For example, if the number of loglike transformed class is more than that of notlog transformed one in a Series, all Samples in the Series are assigned to the loglike transformed class.  If the number of loglike transformed class is equal to that of the notlog transformed one, all Samples are assigned to what they are classified as.  We tested both the original classification model and a combination of the model and voting method on data sets differently classified in a Series (DCS) and entire data set, respectively.  The results are presented in Table 9 with various evaluation measurements.    As a result of classification for all GEO data, 56,012 loglike transformed data and 41,014 notlog transformed data are stored into separated database tables. The integrated database can be queried on the World Wide Web at http://geo. snubi. org/~geoxperanto/html/   GEORetrieval/GEORet_Inter. html (Fig.  4) This interface enables users to search hybridization-centered data.  In comparison with free text search of GEO Entrez Search System, users can search the database efficiently due to partially itemized GEO terms. Some main issues of management in production of microarray experiments are the large amount of information produced and their heterogeneity.  Therefore, it is important to make independently collected microarray data conform to standard for sharing of data efficiently and be comparable with each other.  The MIAME standard can serve both bioinformaticians and biologists to deal with the former issue.  To solve the latter issue, microarray data must be classified by transformation method.  In the process of data classification, we found that a combination of a classification model and other method can boost the performance of classification (Chen et al., 2006).  We have presented a simple but effective two-step method.  It consists of a Logistic Regression model as well as a simple voting method. Recently, it is clear that these datasets should be combined to generate a more comprehensive understanding of underlying biology.  With appropriate integration of heterogeneous microarray data in GEO into the standard-based database, improvement of analysis results and comparison of data from different experiments can be possible.  Integration strategies we proposed allow the GEO to progress remarkably toward a more standardized repository and to serve as a more uniform platform for microarray data analysis.  Also, published research studies using GEO data can be expanded and improved with our database and analysis approaches (Yoon et al., 2006) which have been published.   Yet, we have some problem to solve further.  In data field unification, we can not handle polysemy problem, which means the case that a word has many meanings.  An important next step is to adapt natural language processing method to solve not only the problem but also new terms which will be input in GEO.  In setwise validation of data transformation classification, we assumed that all Samples in a Series have same classification results.  This assumption may miss some case of a false positive result in a Series for entire data set.  With human validation for entire data set, we can correct our validation result.   At the conclusion, we suggest to data submitters that they submit their data with the correct description in the unified format to collaborate with researchers in other fields or to provide machinereadable data for computational postanalysis.  The effort may lead to improve the computational analysis results for the discovery of remarkable biological knowledge. This study was supported by a grant from a Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (20100028631).  YRP’s educational training was supported by the Ministry of Health & Welfare, Republic of Korea (A040002).    '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_root = \"C:/Temp/Genomics-Informatics-Corpus-master (1)/Genomics-Informatics-Corpus-master/raw_text2/\"\n",
    "GNICorpus2 = PlaintextCorpusReader(corpus_root, '.*\\.txt',encoding ='utf-8')\n",
    "raw = GNICorpus2.raw('gni-9-1-19.txt')\n",
    "raw_total = GNICorpus2.raw(GNICorpus2.fileids())\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords : gene expression data, data integration, classification 이부분 삭제하는게 좋을 것 같음\n",
    "다만, classification이랑 뒤에 문장이 붙어있어서 제거가 쉽지 않을 것 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쩜이 없고 공백 두개가 있으면 공백 제거하면 됨 다만, assym- 이랑 channel 문장이 띄어진경우 두가지가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 : 페이지가 넘어갈 때, 문장이 걸쳐있는 경우, 띄어쓰기가 추가로 발생 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of   the', 'as   provided', 'channel   in', '190   examples']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = re.findall(r'[A-z0-9]+   [A-z0-9]+', raw)\n",
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of the', 'as provided', 'channel in', '190 examples']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub(r'  ','',p)for p in p2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 : 페이지가 넘어갈 때, 단어가 걸쳐있는 경우, 띄어쓰기가 추가로 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Display  accession', ' number  was', ' distribution  In', ' asym  metry']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = re.findall(r'[^.][A-z0-9]+  [A-z0-9]+', raw)\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Display-accession', 'number  was', 'distribution  In', 'asymmetry']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = [re.sub(r'asym  metry','asymmetry',p)for p in p1]\n",
    "p1 = [re.sub(r'Display  accession','Display-accession',p)for p in p1]\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 : 링크 띄어쓰기 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www. snubi. org',\n",
       " 'http://mged. sourceforge.  net',\n",
       " 'http://geo. snubi. org']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3 = re.findall(r'http://[A-z]*\\. [A-z]*\\.(?:  | )[A-z]*', raw)\n",
    "p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.snubi.org', 'http://mged.sourceforge.net', 'http://geo.snubi.org']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub(r' ','',p)for p in p3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 : Keywords 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['org/software/GEOQuest/Keywords:']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p4 = re.findall(r'[A-za-z][^.]+Keywords:', raw)\n",
    "p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['org/software/GEOQuest/ Keywords:']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub(r'Keywords:',' Keywords:',p)for p in p4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
