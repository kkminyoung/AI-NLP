{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주어진 임의의 문장 POS 태깅하는 프로그램 작성하기\n",
    "태거(tagger)는 nltk 5.5절의 back off n-gram 태거에 기반하여, 브라운 전체코퍼스를 트레이닝하여 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining taggers with backoff tagging\n",
    "\n",
    "- backoff tagging is one of the core features of SequentialBackoffTagger\n",
    "-  It allows you to chain taggers together so that if one tagger doesn't know how to tag a word, it can pass the word on to the next backofftagger. \n",
    "-  If that one can't do it, it can pass the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. universal Tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(tagset='universal')\n",
    "brown_sents = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51606"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train/test data\n",
    "size = int(len(brown_tagged_sents)*0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger('UNK')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "t3 = nltk.TrigramTagger(train_sents, backoff=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9156346262651662"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245510362314285"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230527440749355"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Trigram 까지 backoff 한 것 보다 Bigram 까지만 backoff 한 것이 성능이 더 좋았다. 따라서 t2까지만 tagging 하는게 더 좋은 방법이 될 수 있겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: time flies like an arrow.\n",
      "Pos Tagging result : NOUN VERB ADP DET NOUN . "
     ]
    }
   ],
   "source": [
    "text = input('Enter a sentence: ')\n",
    "text = nltk.word_tokenize(text) \n",
    "t2.tag(text)\n",
    "print('Pos Tagging result : ',end=\"\")\n",
    "for i in t2.tag(text):\n",
    "    print(i[1], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "->  time flies like an arrow 가 제대로 태깅된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그밖의 예시문장들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: We will see that the tag of a word depends on the word and its context within a sentence. For this reason, we will be working with data at the level of sentences rather than words.\n",
      "Pos Tagging result : PRON VERB VERB ADP DET NOUN ADP DET NOUN VERB ADP DET NOUN CONJ DET NOUN ADP DET NOUN . ADP DET NOUN . PRON VERB VERB VERB ADP NOUN ADP DET NOUN ADP NOUN ADP ADP NOUN . "
     ]
    }
   ],
   "source": [
    "text = input('Enter a sentence: ')\n",
    "text = nltk.word_tokenize(text) \n",
    "t2.tag(text)\n",
    "print('Pos Tagging result : ',end=\"\")\n",
    "for i in t2.tag(text):\n",
    "    print(i[1], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: Tagged corpora use many different conventions for tagging words.\n",
      "Pos Tagging result : UNK UNK NOUN ADJ ADJ NOUN ADP VERB NOUN . "
     ]
    }
   ],
   "source": [
    "text = input('Enter a sentence: ')\n",
    "text = nltk.word_tokenize(text) \n",
    "t2.tag(text)\n",
    "print('Pos Tagging result : ',end=\"\")\n",
    "for i in t2.tag(text):\n",
    "    print(i[1], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. default tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents()\n",
    "brown_sents = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51606"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(len(brown_tagged_sents)*0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1 = nltk.UnigramTagger(train_sents, backoff=nltk.DefaultTagger('NN'))\n",
    "tt2 = nltk.BigramTagger(train_sents, backoff=tt1)\n",
    "tt3 = nltk.TrigramTagger(train_sents, backoff=tt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8912742817627459"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9125751765470128"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9130466670857693"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt3.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 Trigram 까지 backoff 한 것이 성능이 더 좋았다. 따라서 t3까지 tagging 하는게 더 좋은 방법이 될 수 있겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: time flies like an arrow.\n",
      "Pos Tagging result : NN NNS VB AT NN . "
     ]
    }
   ],
   "source": [
    "text = input('Enter a sentence: ')\n",
    "text = nltk.word_tokenize(text) \n",
    "tt3.tag(text)\n",
    "print('Pos Tagging result : ',end=\"\")\n",
    "for i in tt3.tag(text):\n",
    "    print(i[1], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: time flies like an arrow.\n",
      "Pos Tagging result : NN NNS CS AT NN "
     ]
    }
   ],
   "source": [
    "text = input('Enter a sentence: ').split()\n",
    "tt2.tag(text)\n",
    "print('Pos Tagging result : ',end=\"\")\n",
    "for i in tt2.tag(text):\n",
    "    print(i[1], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> time flies like an arrow 가 제대로 태깅 되지 않은 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그밖의 예시문장들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: In the beginning God created the heaven and the earth. And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters. And God said, Let there be light: and there was light. And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day. And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so. Thus the heavens and the earth were finished, and all the host of them. But there went up a mist from the earth, and watered the whole face of the ground.  \n",
      "Pos Tagging result : IN AT NN NP VBD AT NN CC AT NN CC AT NN BEDZ IN NN CC NN CC NN BEDZ IN AT NN IN AT NN CC AT NN-TL IN-TL NP-TL VBD IN AT NN IN AT NN CC NP NN VB RB BE NN CC EX BEDZ NN CC NP VBD AT NN NN CC AT NN PPS VBD NN CC AT NN CC AT NN BED AT OD NN CC NP VBD AT NN CC VBN AT NNS WDT BED IN AT NN IN AT NNS WDT BED IN AT NN CC PPS BEDZ NN RB AT NNS CC AT NN BED NN CC ABN AT NN IN NN CC EX VBD RP AT NN IN AT NN CC VBD AT JJ NN IN AT NN "
     ]
    }
   ],
   "source": [
    "text = input('Enter a sentence: ').split()\n",
    "tt2.tag(text)\n",
    "print('Pos Tagging result : ',end=\"\")\n",
    "for i in tt2.tag(text):\n",
    "    print(i[1], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  문제점 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 어떤 단어를 명사로 표기하기로 결정했지만, 나중에 그것이 동사였어야 했다는 증거를 찾아낸다면, 다시 돌아가서 우리의 실수를 고칠 수 없다.\n",
    "\n",
    " -> 해결책 : 가능한 모든 시퀀스에 점수를 할당하고 전체 점수가 가장 높은 시퀀스를 선택함. (히든 마르코프 모델들이 취한 접근법)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### + data sparseness problem of N-gram Tagger\n",
    "\n",
    "1. n-gram table의 사이즈 문제\n",
    "\n",
    "n을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각해짐. 또한 n이 커질수록 모델 사이즈가 커진다는 문제점도 있음. 기본적으로 코퍼스의 모든 n-gram에 대해서 카운트를 해야 하기때문\n",
    "n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어짐\n",
    "\n",
    "2. context : n-gram tagger\n",
    "\n",
    "the only information an n-gram tagger considers from prior context is tags, even though words themselves might be a useful source of information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
